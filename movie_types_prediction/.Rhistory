movie
library(dplyr)
library(haven)
# loading the dataset
movie <- read_sas("movies.sas7bdat")
View(movie)
#selecting useful variables and renaming
movie <- movie %>% select(Type, Year, Domestic__, Worldwide__)
movie <- movie %>% rename(movie_type = Type, domestic_sale = Domestic__,
worldwide_sale = Worldwide__)
# change year datatype
movie$Year <- as.integer(movie$Year)
library(tidyverse)
p <- movie %>% ggplot(aes(movie_type, domestic_sale, fill = movie_type)) + geom_boxplot()
p
# scatter plot
plot(domestic_sale ~ worldwide_sale, movie)
with(movie,text(domestic_sale~worldwide_sale, labels = movie_type, pos =4, cex=.20))
movie_dat <- (movie[, 3:4] na.rm = TRUE) # taking only the quantitative columns
movie_dat <- movie[, 3:4] # taking only the quantitative columns
movie_dat
# loading the dataset
movie <- read_sas("movies.sas7bdat", na.rm = TRUE)
# loading the dataset
movie <- read_sas("movies.sas7bdat")
#------------------------------------------------------------
sta <- movie %>% group_by(movie_type) %>% summarise(mean = mean(domestic_sale), sd = sd(domestic_sale, na.rm = TRUE))
# loading the dataset
movie <- read_sas("movies.sas7bdat")
View(movie)
#checking for colnms
names(movie)
#selecting useful variables and renaming
movie <- movie %>% select(Type, Year, Domestic__, Worldwide__)
View(movie)
movie <- movie %>% rename(movie_type = Type, domestic_sale = Domestic__,
worldwide_sale = Worldwide__)
# change year datatype
movie$Year <- as.integer(movie$Year)
library(tidyverse)
p <- movie %>% ggplot(aes(movie_type, domestic_sale, fill = movie_type)) + geom_boxplot()
p
# scatter plot
plot(domestic_sale ~ worldwide_sale, movie)
with(movie,text(domestic_sale~worldwide_sale, labels = movie_type, pos =4, cex=.20))
movie_dat <- movie[, 3:4] # taking only the quantitative columns
movie_dat
# scale the data
movie_data_scal <- data.frame(scale(movie_dat))
movie_data_scal
movie_dat1 <- movie_data_scal
movie_dat1
# calculate the eclidean distance
distance <- dist(movie_data_scal)
distance
# determine the value of K where k is optimal, i.e cal how amny clusters we need
# wss= within sum of squares
library(factoextra)
fviz_nbclust(movie_data_scal, kmeans, method = "wss") + labs(subtitle = "elbow plot")
# kmeans
km <- kmeans(movie_data_scal, centers = 3)
print(km)
km$cluster
#visualize the clusters
km.cluster <- km$cluster
rownames(movie_data_scal) <- paste(movie$movie_type, 1:dim(movie)[1], sep = "_")
fviz_cluster(list(movie_data_scal, cluster = km.cluster))
table(km.cluster, movie$movie_type)
movie_dat1$cluster <- km.cluster
movie_dat1
trainingindex <- createDataPartition(movie_dat1$cluster, p=0.8, list = FALSE)
# building the predictive model
library(caret)
set.seed(20000)
trainingindex <- createDataPartition(movie_dat1$cluster, p=0.8, list = FALSE)
trainingset <- movie_dat1[trainingindex,]
testset <- movie_dat[-trainingindex,]
traingset
trainingset <- movie_dat1[trainingindex,]
testset <- movie_dat1[-trainingindex,]
traingset
trainingset
control <- trainControl(method = "cv", number = 10, p=.9)
train_knn_cv <- train(factor(cluster) ~., method = "knn", data = trainingset,
tuneGrid = data.frame(k=c(3,5,7)), trControl = control)
#apply model for prediction
model.training <- predict(train_knn_cv, trainingset)
model.testing <- predict(train_knn_cv, testset)
# cross validation
cm <- confusionMatrix(model.training, factor(traingset$cluster))  # confusion matrix for the trainingset
# cross validation
cm <- confusionMatrix(model.training, factor(trainingset$cluster))  # confusion matrix for the trainingset
cm
cm$overall["Accuracy"]
test_cm <- confusionMatrix(model.testing, factor(testset$cluster))
test_cm
test_cm$overall["Accuracy"]
library(tidyverse)
trainingindex <- createDataPartition(movie_dat1$cluster, p=0.8, list = FALSE, na.rm = TRUE)
library(dplyr)
library(haven)
# loading the dataset
movie <- read_sas("movies.sas7bdat")
View(movie)
str(movie)
#selecting useful variables and renaming
movie <- movie %>% select(Type, Year, Domestic__, Worldwide__)
movie <- movie %>% rename(movie_type = Type, domestic_sale = Domestic__,
worldwide_sale = Worldwide__)
# change year datatype
movie$Year <- as.integer(movie$Year)
sapply(movie, class)
#------------------------------------------------------------
sta <- movie %>% group_by(movie_type) %>% summarise(mean = mean(domestic_sale), sd = sd(domestic_sale, na.rm = TRUE))
p <- movie %>% ggplot(aes(movie_type, domestic_sale, fill = movie_type)) + geom_boxplot()
# scatter plot
plot(domestic_sale ~ worldwide_sale, movie)
with(movie,text(domestic_sale~worldwide_sale, labels = movie_type, pos =4, cex=.20))
movie_dat <- movie[, 3:4] # taking only the quantitative columns
movie_dat
# scale the data
movie_data_scal <- scale(movie_dat)
movie_data_scal
# calculate the eclidean distance
distance <- dist(movie_data_scal)
distance
#----Hierarchical clustering using dendrogram------------
# cluster Dendrogram with complete linkage:
hc.c <- hclust(distance)
hc.c
plot(hc.c, cex = 0.85, main = "", xlab = "")
plot(hc.c, labels = movie$movie_type, hang = -1)  # cluster dendrogram
# silhouette plot
library(cluster)
plot(silhouette(cutree(hc.c,3), distance))
fviz_nbclust(movie_data_scal, kmeans, method = "wss") + labs(subtitle = "elbow plot")
# kmeans
km <- kmeans(movie_data_scal, centers = 3)
print(km)
km$cluster
#visualize the clusters
km.cluster <- km$cluster
rownames(movie_data_scal) <- paste(movie$movie_type, 1:dim(movie)[1], sep = "_")
fviz_cluster(list(movie_data_scal, cluster = km.cluster))
table(km.cluster, movie$movie_type)
movie_dat$cluster <- km.cluster
movie_dat
# building the predictive model
library(caret)
set.seed(20000)
trainingindex <- createDataPartition(movie_dat$cluster, p=0.8, list = FALSE)
trainingset <- movie_dat[trainingindex,]
testset <- movie_dat[-trainingindex,]
trainingset
set.seed(2344)
movie_dat$cluster <- factor(movie_dat$cluster)
movie_dat$cluster <- factor(movie_dat$cluster)
trainingindex <- createDataPartition(movie_dat$cluster, p=0.8, list = FALSE)
trainingset <- movie_dat[trainingindex,]
testset <- movie_dat[-trainingindex,]
trainingset
control <- trainControl(method = "cv", number = 10, p=.9)
train_knn_cv <- train(cluster ~., method = "knn", data = trainingset,
tuneGrid = data.frame(k=c(3,5,7)),
trControl = control,
preProc = c("center", "scale"))
fit_knn_cv <- train(cluster ~., method = "knn", data = trainingset,
tuneGrid = data.frame(k=c(3,5,7)),
trControl = control,
preProc = c("center", "scale"))
fit_knn_cv
plot(fit_knn_cv, highlight = TRUE)
# looking at variable importance
varImp(fit_knn_cv)
# looking at variable importance
varImp(fit_knn_cv, rm.na + TRUE)
# looking at variable importance
varImp(fit_knn_cv, rm.na + TRUE)
is.atomic(fit_knn_cv)
# looking at variable importance
varImp(fit_knn_cv, na.rm = TRUE)
# looking at variable importance
varImp(fit_knn_cv,mean, na.rm = TRUE)
# looking at variable importance
varImp(fit_knn_cv, scale = FALSE)
# looking at variable importance
varImp(fit_knn_cv, scale = FALSE)
plot(fit_knn_cv, top =2)
plot(fit_knn_cv, top =2)
# looking at variable importance
varImp(fit_knn_cv)
#apply model for prediction
model.training <- predict(fit_knn_cv, trainingset)
model.testing <- predict(fit_knn_cv, testset)
# cross validation
cm <- confusionMatrix(model.training, factor(trainingset$cluster))  # confusion matrix for the trainingset
cm
cm$overall["Accuracy"]
# looking at variable importance
varImp(fit_knn_cv)
fit_knn_cv
imp <- importance(fit_knn_cv)
# looking at variable importance
varImp(fit_knn_cv)
set.seed(2344)
control <- trainControl(method = "cv", number = 10, p=.9)
fit_knn_cv <- train(cluster ~., method = "knn", data = trainingset,
tuneGrid = data.frame(k=seq(3,33,2)),
trControl = control,
preProc = c("center", "scale"))
fit_knn_cv
plot(fit_knn_cv, highlight = TRUE)
# looking at variable importance
varImp(fit_knn_cv)
class(fit_knn_cv)
#apply model for prediction
pred <- predict(fit_knn_cv, testset)
pred
test_cm <- confusionMatrix(pred, testset$cluster)
test_cm
test_cm$overall["Accuracy"]
# random forest
library(randomForest)
# random forest
install.packages("randomForest")
library(randomForest)
contro1 <- trainControl(method = "cv", number = 10, p=.9)
set.seed(2333)
fit_rf <- randomForest(cluster ~., data = trainingset)
fit_rf
varImp(fit_rf)
# apply the model to make prediction
pred1 <- predict(fit_rf, testset)
pred1
plot(fit_rf)
#confusion matrix
confu <- confusionMatrix(pred1, testset$cluster)
confu
# overall accuracy
confu$overall["Accuracy"]
